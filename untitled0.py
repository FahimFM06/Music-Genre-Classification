# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pNn6cbd5XNwR65ySLYnl9sxzeScCKchc
"""

# prompt: connect to drive
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/TUD/Projects/'Music Genre Classification'/Data/genres_original/
# %ls

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import librosa
import librosa.display
from collections import Counter

import os
import librosa
import warnings

# Root directory where your genre folders are located
DATA_DIR = "/content/drive/My Drive/TUD/Projects/Music Genre Classification/Data/genres_original"

# List all genres (folders)
genres = os.listdir(DATA_DIR)
print("Genres:", genres)

# Arrays to store everything
audio_signals = []
sample_rates = []
labels = []
file_paths = []

# Loop through each genre folder
for genre in genres:
    genre_path = os.path.join(DATA_DIR, genre)
    if not os.path.isdir(genre_path):
        continue

    print(f"Loading: {genre}")

    for file in os.listdir(genre_path):
        if file.endswith(".wav"):
            file_path = os.path.join(genre_path, file)

            try:
                # Load the audio file
                signal, sr = librosa.load(file_path, sr=None)   # keep original sample rate

                audio_signals.append(signal)
                sample_rates.append(sr)
                labels.append(genre)
                file_paths.append(file_path)
            except Exception as e:
                warnings.warn(f"Could not load {file_path}: {e}")
                continue # Skip to the next file if loading fails

print("Total audio files loaded:", len(audio_signals))

import matplotlib.pyplot as plt
import librosa.display

idx = 0  # choose any index
plt.figure(figsize=(12, 4))
librosa.display.waveshow(audio_signals[idx], sr=sample_rates[idx])
plt.title(f"Waveform - {labels[idx]}")
plt.show()

import pandas as pd

# Compute duration (in seconds) for each track
durations = [len(x) / sr for x, sr in zip(audio_signals, sample_rates)]

data = pd.DataFrame({
    "file_path": file_paths,
    "genre": labels,
    "sample_rate": sample_rates,
    "duration_sec": durations
})

data.head()

print("Total tracks:", len(data))
print("Unique genres:", data['genre'].nunique())
print("\nTracks per genre:")
print(data['genre'].value_counts())

plt.figure(figsize=(8, 4))
data['genre'].value_counts().plot(kind='bar')
plt.title("Number of Tracks per Genre")
plt.xlabel("Genre")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

print("Sample rate distribution:")
print(data['sample_rate'].value_counts())

plt.figure(figsize=(6, 4))
data['sample_rate'].value_counts().plot(kind='bar')
plt.title("Sample Rate Distribution")
plt.xlabel("Sample Rate (Hz)")
plt.ylabel("Count")
plt.tight_layout()
plt.show()

print(data['duration_sec'].describe())

plt.figure(figsize=(8, 4))
plt.hist(data['duration_sec'], bins=20, edgecolor='black')
plt.title("Distribution of Track Duration")
plt.xlabel("Duration (seconds)")
plt.ylabel("Number of Tracks")
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 5))
data.boxplot(column='duration_sec', by='genre', grid=False)
plt.title("Track Duration by Genre")
plt.suptitle("")  # remove automatic super title
plt.xlabel("Genre")
plt.ylabel("Duration (seconds)")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

genres = sorted(data['genre'].unique())

for g in genres:
    # pick first track of this genre
    row = data[data['genre'] == g].iloc[0]
    sr = row['sample_rate']
    signal, _ = librosa.load(row['file_path'], sr=sr)

    plt.figure(figsize=(10, 3))
    librosa.display.waveshow(signal, sr=sr)
    plt.title(f"Waveform example - {g}")
    plt.xlabel("Time (s)")
    plt.tight_layout()
    plt.show()

import numpy as np
for g in genres[:4]:  # first 4 genres as example
    row = data[data['genre'] == g].iloc[0]
    sr = row['sample_rate']
    signal, _ = librosa.load(row['file_path'], sr=sr)

    # create mel spectrogram
    S = librosa.feature.melspectrogram(y=signal, sr=sr, n_mels=128)
    S_db = librosa.power_to_db(S, ref=np.max)

    plt.figure(figsize=(8, 4))
    librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='mel')
    plt.title(f"Mel Spectrogram - {g}")
    plt.colorbar(format='%+2.0f dB')
    plt.tight_layout()
    plt.show()

# Just based on path names
print("Duplicate file paths:", data['file_path'].duplicated().sum())

# If you want to check duplicates by file name only:
data['file_name'] = data['file_path'].apply(lambda x: os.path.basename(x))
print("Duplicate file names:", data['file_name'].duplicated().sum())

import os
import hashlib
import numpy as np
import librosa
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Root folder with the 10 genre subfolders
DATA_DIR = "/content/drive/My Drive/TUD/Projects/Music Genre Classification/Data/genres_original"

# Audio parameters
TARGET_SR = 22050          # target sample rate
TRACK_DURATION = 30.0      # seconds (GTZAN tracks are ~30s)
SAMPLES_PER_TRACK = int(TARGET_SR * TRACK_DURATION)

# Feature (log-mel spectrogram) parameters
N_FFT = 2048
HOP_LENGTH = 512
N_MELS = 128

# Segment parameters (we split each 30s track into 3s segments)
SEGMENT_DURATION = 3.0
SEGMENT_SAMPLES = int(TARGET_SR * SEGMENT_DURATION)
SEGMENTS_PER_TRACK = int(SAMPLES_PER_TRACK / SEGMENT_SAMPLES)  # 10 segments

# ============================
# 1. Remove duplicate audio files
# ============================

def file_hash(filepath):
    """Return SHA-256 hash of a file (binary content)."""
    hasher = hashlib.sha256()
    with open(filepath, "rb") as f:
        buf = f.read()
        hasher.update(buf)
    return hasher.hexdigest()


hash_dict = {}       # hash -> first file path
duplicates = []      # list of duplicate file paths

for genre in os.listdir(DATA_DIR):
    genre_path = os.path.join(DATA_DIR, genre)
    if not os.path.isdir(genre_path):
        continue

    for f in os.listdir(genre_path):
        if f.endswith(".wav"):
            fp = os.path.join(genre_path, f)
            h = file_hash(fp)

            if h not in hash_dict:
                hash_dict[h] = fp
            else:
                duplicates.append(fp)

print("Total duplicate files found:", len(duplicates))

# (Safer) move duplicates to a backup folder instead of deleting outright
import shutil

DUP_DIR = os.path.join(os.path.dirname(DATA_DIR), "duplicates_backup")
os.makedirs(DUP_DIR, exist_ok=True)

for d in duplicates:
    shutil.move(d, os.path.join(DUP_DIR, os.path.basename(d)))

print("Duplicates moved to:", DUP_DIR)

# ============================
# 2. Collect track-level file paths and labels
# ============================

file_paths = []
track_labels = []

genres = sorted(os.listdir(DATA_DIR))
print("Genres found:", genres)

for genre in genres:
    genre_path = os.path.join(DATA_DIR, genre)
    if not os.path.isdir(genre_path):
        continue

    for f in os.listdir(genre_path):
        if f.endswith(".wav"):
            file_paths.append(os.path.join(genre_path, f))
            track_labels.append(genre)

file_paths = np.array(file_paths)
track_labels = np.array(track_labels)

print("Total tracks after duplicate removal:", len(file_paths))

# ============================
# 3. Train / validation / test split (track-wise)
# ============================

train_paths, temp_paths, train_labels, temp_labels = train_test_split(
    file_paths,
    track_labels,
    test_size=0.30,             # 70% train, 30% temp
    stratify=track_labels,
    random_state=42
)

val_paths, test_paths, val_labels, test_labels = train_test_split(
    temp_paths,
    temp_labels,
    test_size=0.50,             # 15% val, 15% test overall
    stratify=temp_labels,
    random_state=42
)

print("Tracks per split:")
print("Train:", len(train_paths))
print("Val:  ", len(val_paths))
print("Test: ", len(test_paths))

# ============================
# 4. Audio loading & feature extraction functions
# ============================

def load_fixed_length(path,
                      target_sr=TARGET_SR,
                      track_samples=SAMPLES_PER_TRACK):
    """
    Load audio, resample, and force to fixed length (pad or trim).
    Returns: 1D numpy array, or None if loading fails.
    """
    try:
        signal, sr = librosa.load(path, sr=target_sr)
    except Exception as e:
        warnings.warn(f"Could not load {path} for feature extraction: {e}")
        return None

    if len(signal) > track_samples:
        signal = signal[:track_samples]
    elif len(signal) < track_samples:
        pad_width = track_samples - len(signal)
        signal = np.pad(signal, (0, pad_width), mode="constant")

    return signal


def extract_mel_segments(signal,
                         sr=TARGET_SR,
                         n_fft=N_FFT,
                         hop_length=HOP_LENGTH,
                         n_mels=N_MELS,
                         segment_samples=SEGMENT_SAMPLES,
                         segments_per_track=SEGMENTS_PER_TRACK):
    """
    Split a full-track signal into fixed-length segments and
    convert each segment to a standardized log-mel spectrogram.

    Returns: array of shape (segments_per_track, n_mels, time_frames)
    """
    segments = []

    for s in range(segments_per_track):
        start = s * segment_samples
        end = start + segment_samples
        segment = signal[start:end]

        mel = librosa.feature.melspectrogram(
            y=segment,
            sr=sr,
            n_fft=n_fft,
            hop_length=hop_length,
            n_mels=n_mels,
        )
        log_mel = librosa.power_to_db(mel, ref=np.max)

        # Normalize each segment (zero mean, unit variance)
        mean = log_mel.mean()
        std = log_mel.std() + 1e-9
        log_mel = (log_mel - mean) / std

        segments.append(log_mel)

    return np.array(segments)


def paths_to_mel_dataset(paths, labels):
    """
    Convert a list of file paths to:
    X: all log-mel segments
    y: labels repeated for each segment
    """
    X_list, y_list = [], []

    for path, label in zip(paths, labels):
        signal = load_fixed_length(path)
        if signal is None:  # Skip if loading failed
            continue
        mel_segments = extract_mel_segments(signal)

        for seg in mel_segments:
            X_list.append(seg)
            y_list.append(label)

    return np.array(X_list), np.array(y_list)

# ============================
# 5. Build datasets (X, y) for each split
# ============================

X_train, y_train = paths_to_mel_dataset(train_paths, train_labels)
X_val,   y_val   = paths_to_mel_dataset(val_paths,   val_labels)
X_test,  y_test  = paths_to_mel_dataset(test_paths,  test_labels)

print("Raw shapes (without channel dim):")
print("X_train:", X_train.shape, "y_train:", y_train.shape)
print("X_val:  ", X_val.shape,   "y_val:  ", y_val.shape)
print("X_test: ", X_test.shape,  "y_test: ", y_test.shape)

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks, optimizers
import numpy as np
import seaborn as sns

# 1. Encode labels into integers 0..num_classes-1
label_encoder = LabelEncoder()
label_encoder.fit(np.concatenate([y_train, y_val, y_test]))

y_train_enc = label_encoder.transform(y_train)
y_val_enc   = label_encoder.transform(y_val)
y_test_enc  = label_encoder.transform(y_test)

idx_to_genre = {i: g for i, g in enumerate(label_encoder.classes_)}
print("Label mapping:", idx_to_genre)

# 2. Add channel dimension for CNN/CRNN/Transformer (last dim = 1 channel)
X_train_cnn = X_train[..., np.newaxis]
X_val_cnn   = X_val[..., np.newaxis]
X_test_cnn  = X_test[..., np.newaxis]

print("X_train_cnn:", X_train_cnn.shape, "y_train_enc:", y_train_enc.shape)
print("X_val_cnn:  ", X_val_cnn.shape,   "y_val_enc:  ", y_val_enc.shape)
print("X_test_cnn: ", X_test_cnn.shape,  "y_test_enc: ", y_test_enc.shape)

# 3. Update global variables for later code
num_classes = len(label_encoder.classes_)
input_shape = X_train_cnn.shape[1:]  # (n_mels, time_frames, 1)


# =========================================================================
# Utility functions for model compilation, training, and evaluation
# =========================================================================

def compile_model(model, lr=1e-3):
    """Compiles a Keras model."""
    model.compile(
        optimizer=optimizers.Adam(learning_rate=lr),
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"],
    )
    return model

def train_model(model, model_name, X_train, y_train, X_val, y_val, batch_size=32, epochs=50):
    """Trains a Keras model with callbacks."""
    print(f"\nTraining {model_name}...")

    # Callbacks
    early_stopping = callbacks.EarlyStopping(
        monitor="val_accuracy", patience=10, restore_best_weights=True
    )
    model_checkpoint = callbacks.ModelCheckpoint(
        f"{model_name}_best_model.h5",
        monitor="val_accuracy",
        save_best_only=True,
        verbose=0,
    )

    history = model.fit(
        X_train,
        y_train,
        validation_data=(X_val, y_val),
        batch_size=batch_size,
        epochs=epochs,
        callbacks=[early_stopping, model_checkpoint],
        verbose=1,
    )

    best_model = models.load_model(f"{model_name}_best_model.h5")
    return best_model, history

def evaluate_model(model, X_test, y_test, idx_to_genre, title_suffix=""):
    """Evaluates a Keras model and prints classification report/confusion matrix."""
    print(f"\nEvaluating model{title_suffix}...")

    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
    print(f"Test Loss: {loss:.4f}")
    print(f"Test Accuracy: {accuracy:.4f}")

    y_pred_probs = model.predict(X_test)
    y_pred = np.argmax(y_pred_probs, axis=1)

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=list(idx_to_genre.values())))

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        cm,
        annot=True,
        fmt="d",
        cmap="Blues",
        xticklabels=list(idx_to_genre.values()),
        yticklabels=list(idx_to_genre.values()),
    )
    plt.title(f"Confusion Matrix{title_suffix}")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.show()

    return loss, accuracy

import tensorflow as tf
from tensorflow.keras import layers, models # Import models

@tf.keras.utils.register_keras_serializable()
def to_sequence(tensor):
    """
    Converts a tensor from (batch, H, W, C) to (batch, W, H*C)
    for use in an RNN layer.
    """
    # tensor: (batch, H, W, C)
    tensor = tf.transpose(tensor, perm=[0, 2, 1, 3])  # (batch, W, H, C) -> time = W
    shape = tf.shape(tensor)
    batch_size = shape[0]
    time_steps = shape[1]
    features = shape[2] * shape[3]
    return tf.reshape(tensor, [batch_size, time_steps, features])

def build_crnn_model(input_shape, num_classes):
    inputs = tf.keras.layers.Input(shape=input_shape)  # (n_mels, time, 1)

    x = tf.keras.layers.Conv2D(32, (3, 3), padding="same")(inputs)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Activation("relu")(x)
    x = tf.keras.layers.MaxPooling2D((2, 2))(x)
    x = tf.keras.layers.Dropout(0.25)(x)

    x = tf.keras.layers.Conv2D(64, (3, 3), padding="same")(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Activation("relu")(x)
    x = tf.keras.layers.MaxPooling2D((2, 2))(x)
    x = tf.keras.layers.Dropout(0.30)(x)

    x = tf.keras.layers.Conv2D(128, (3, 3), padding="same")(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Activation("relu")(x)
    x = tf.keras.layers.MaxPooling2D((2, 2))(x)
    x = tf.keras.layers.Dropout(0.35)(x)

    # Convert feature map to sequence: (batch, time_steps, features)
    # The to_sequence function is now globally defined and registered.
    x = tf.keras.layers.Lambda(to_sequence)(x)

    # BiLSTM
    x = tf.keras.layers.Bidirectional(
        tf.keras.layers.LSTM(128, return_sequences=False)
    )(x)
    x = tf.keras.layers.Dropout(0.5)(x)

    # Dense head
    x = tf.keras.layers.Dense(128, activation="relu")(x)
    x = tf.keras.layers.Dropout(0.5)(x)
    outputs = tf.keras.layers.Dense(num_classes, activation="softmax")(x)

    model = tf.keras.models.Model(inputs=inputs, outputs=outputs, name="CRNN")
    return model

crnn_model = build_crnn_model(input_shape, num_classes)
crnn_model.summary()

# ========================================================
# 1. Compile CRNN model
# ========================================================
crnn_model = compile_model(crnn_model, lr=1e-3)

# ========================================================
# 2. Define NEW callbacks (save to Google Drive)
# ========================================================

drive_model_path = "/content/drive/My Drive/TUD/Projects/Music Genre Classification/best_crnn_model.keras"

early_stopping_cb = callbacks.EarlyStopping(
    monitor="val_loss",
    patience=7,
    restore_best_weights=True,
    verbose=1
)

model_checkpoint_cb = callbacks.ModelCheckpoint(
    filepath=drive_model_path,
    monitor="val_accuracy",
    save_best_only=True,
    save_weights_only=False,
    verbose=1
)

reduce_lr_cb = callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.5,
    patience=3,
    min_lr=1e-6,
    verbose=1
)

crnn_callbacks = [early_stopping_cb, model_checkpoint_cb, reduce_lr_cb]

# ========================================================
# 3. Train CRNN with early stopping + save best model
# ========================================================
crnn_history = crnn_model.fit(
    X_train_cnn,
    y_train_enc,
    validation_data=(X_val_cnn, y_val_enc),
    epochs=60,
    batch_size=32,
    callbacks=crnn_callbacks,
    verbose=1
)

# ========================================================
# 4. Load best model from Google Drive
# ========================================================
crnn_best = tf.keras.models.load_model(drive_model_path)

# ========================================================
# 5. Evaluate CRNN best model
# ========================================================
crnn_test_loss, crnn_test_acc = evaluate_model(
    crnn_best,
    X_test_cnn,
    y_test_enc,
    idx_to_genre=idx_to_genre,
    title_suffix="(CRNN)"
)

print("CRNN test accuracy:", crnn_test_acc)
print("Best CRNN model saved at:", drive_model_path)

#Option B â€“ CRNN (CNN + Bi-LSTM / GRU)

import os
import copy
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt

# If your y_train, y_val, y_test are strings (genres), encode them:
le = LabelEncoder()
le.fit(np.concatenate([y_train, y_val, y_test]))

y_train_enc = le.transform(y_train)
y_val_enc   = le.transform(y_val)
y_test_enc  = le.transform(y_test)

idx_to_genre = {i: g for i, g in enumerate(le.classes_)}
num_classes = len(le.classes_)

print("Classes:", idx_to_genre)

# X_train is (N, n_mels, time_frames). Convert to float32.
X_train_np = X_train.astype(np.float32)
X_val_np   = X_val.astype(np.float32)
X_test_np  = X_test.astype(np.float32)

class MelDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.from_numpy(X)              # (N, n_mels, time)
        self.y = torch.from_numpy(y).long()       # (N,)

    def __len__(self):
        return self.X.shape[0]

    def __getitem__(self, idx):
        x = self.X[idx]   # (n_mels, time)
        y = self.y[idx]
        return x, y

batch_size = 32

train_ds = MelDataset(X_train_np, y_train_enc)
val_ds   = MelDataset(X_val_np,   y_val_enc)
test_ds  = MelDataset(X_test_np,  y_test_enc)

train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)
val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)
test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)

class CRNN_BiGRU(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        # CNN feature extractor
        self.cnn = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d((2, 2)),
            nn.Dropout(0.25),

            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d((2, 2)),
            nn.Dropout(0.30),

            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d((2, 2)),
            nn.Dropout(0.35),
        )

        # Bi-GRU (input_size will be determined dynamically)
        self.gru = None
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(256, 128),   # will be updated after building GRU
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, num_classes)
        )

    def _build_gru_if_needed(self, x):
        # x is CNN output: (batch, C, H, W)
        # We want sequence length = W, features = C*H
        b, c, h, w = x.shape
        feat_dim = c * h
        if self.gru is None:
            self.gru = nn.GRU(
                input_size=feat_dim,
                hidden_size=128,
                num_layers=1,
                batch_first=True,
                bidirectional=True
            ).to(x.device)
            # Update first Linear input dim (BiGRU output dim = 2*hidden_size = 256)
            self.classifier[1] = nn.Linear(256, 128).to(x.device)

    def forward(self, x):
        # x: (batch, n_mels, time)
        x = x.unsqueeze(1)         # (batch, 1, n_mels, time)
        x = self.cnn(x)            # (batch, C, H, W)

        self._build_gru_if_needed(x)

        b, c, h, w = x.shape
        x = x.permute(0, 3, 1, 2).contiguous()     # (batch, W, C, H)
        x = x.view(b, w, c * h)                    # (batch, time_steps, features)

        out, _ = self.gru(x)                       # (batch, time_steps, 256)
        out = out[:, -1, :]                        # last time step (batch, 256)

        logits = self.classifier(out)              # (batch, num_classes)
        return logits

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

model = CRNN_BiGRU(num_classes=num_classes).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

save_path = "/content/drive/My Drive/TUD/Projects/Music Genre Classification/best_crnn_bigru_pytorch.pt"

patience = 7
best_val_acc = 0.0
epochs_no_improve = 0
best_state = None

def run_epoch(loader, train=True):
    if train:
        model.train()
    else:
        model.eval()

    total_loss = 0.0
    correct = 0
    total = 0

    with torch.set_grad_enabled(train):
        for xb, yb in loader:
            xb = xb.to(device, non_blocking=True)
            yb = yb.to(device, non_blocking=True)

            logits = model(xb)
            loss = criterion(logits, yb)

            if train:
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

            total_loss += loss.item() * xb.size(0)
            preds = torch.argmax(logits, dim=1)
            correct += (preds == yb).sum().item()
            total += xb.size(0)

    return total_loss / total, correct / total

num_epochs = 60

for epoch in range(1, num_epochs + 1):
    train_loss, train_acc = run_epoch(train_loader, train=True)
    val_loss, val_acc     = run_epoch(val_loader, train=False)

    print(f"Epoch {epoch:02d}/{num_epochs} "
          f"train_loss={train_loss:.4f} train_acc={train_acc:.4f} "
          f"val_loss={val_loss:.4f} val_acc={val_acc:.4f}")

    # Save best
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        best_state = copy.deepcopy(model.state_dict())
        torch.save(best_state, save_path)
        print(f"  Saved best model to: {save_path}")
        epochs_no_improve = 0
    else:
        epochs_no_improve += 1

    # Early stopping
    if epochs_no_improve >= patience:
        print(f"Early stopping triggered. Best val_acc={best_val_acc:.4f}")
        break

# Load best weights
model.load_state_dict(torch.load(save_path, map_location=device))
print("Loaded best model from:", save_path)

model.eval()
all_preds = []
all_true = []

with torch.no_grad():
    for xb, yb in test_loader:
        xb = xb.to(device, non_blocking=True)
        logits = model(xb)
        preds = torch.argmax(logits, dim=1).cpu().numpy()
        all_preds.append(preds)
        all_true.append(yb.numpy())

y_pred = np.concatenate(all_preds)
y_true = np.concatenate(all_true)

test_acc = (y_pred == y_true).mean()
print(f"Test Accuracy (PyTorch CRNN Bi-GRU): {test_acc:.4f}")

target_names = [idx_to_genre[i] for i in sorted(idx_to_genre)]
print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=target_names))

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(10, 8))
plt.imshow(cm, interpolation="nearest")
plt.title("Confusion Matrix (PyTorch CRNN Bi-GRU)")
plt.colorbar()
plt.xlabel("Predicted")
plt.ylabel("True")
plt.tight_layout()
plt.show()

#Spectrogram Transformer-like model

import os
import copy
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt

# Encode labels to integers
le = LabelEncoder()
le.fit(np.concatenate([y_train, y_val, y_test]))

y_train_enc = le.transform(y_train)
y_val_enc   = le.transform(y_val)
y_test_enc  = le.transform(y_test)

idx_to_genre = {i: g for i, g in enumerate(le.classes_)}
num_classes = len(le.classes_)

print("Classes:", idx_to_genre)

X_train_np = X_train.astype(np.float32)
X_val_np   = X_val.astype(np.float32)
X_test_np  = X_test.astype(np.float32)

class MelDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.from_numpy(X)        # (N, n_mels, time)
        self.y = torch.from_numpy(y).long() # (N,)

    def __len__(self):
        return self.X.shape[0]

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

batch_size = 32

train_ds = MelDataset(X_train_np, y_train_enc)
val_ds   = MelDataset(X_val_np,   y_val_enc)
test_ds  = MelDataset(X_test_np,  y_test_enc)

train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)
val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)
test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)

class SpecPatchEmbedding(nn.Module):
    def __init__(self, embed_dim=128, patch_size=(16, 16)):
        super().__init__()
        self.patch_h, self.patch_w = patch_size
        self.embed_dim = embed_dim
        # projection after flattening each patch
        self.proj = nn.Linear(self.patch_h * self.patch_w, embed_dim)

    def forward(self, x):
        """
        x: (B, H, W)  where H=n_mels, W=time_frames
        returns: (B, N_patches, embed_dim)
        """
        B, H, W = x.shape

        # Pad H and W to be divisible by patch sizes
        pad_h = (self.patch_h - (H % self.patch_h)) % self.patch_h
        pad_w = (self.patch_w - (W % self.patch_w)) % self.patch_w

        if pad_h > 0 or pad_w > 0:
            x = nn.functional.pad(x, (0, pad_w, 0, pad_h))  # (left,right,top,bottom) for 2D? here (W) then (H)
            H = H + pad_h
            W = W + pad_w

        # Create patches
        new_h = H // self.patch_h
        new_w = W // self.patch_w

        # (B, new_h, patch_h, new_w, patch_w)
        x = x.view(B, new_h, self.patch_h, new_w, self.patch_w)
        # (B, new_h, new_w, patch_h, patch_w)
        x = x.permute(0, 1, 3, 2, 4).contiguous()
        # (B, N_patches, patch_h*patch_w)
        x = x.view(B, new_h * new_w, self.patch_h * self.patch_w)

        # Project each patch
        x = self.proj(x)  # (B, N_patches, embed_dim)
        return x


class SpectrogramTransformer(nn.Module):
    def __init__(self,
                 num_classes,
                 embed_dim=128,
                 patch_size=(16, 16),
                 num_layers=4,
                 num_heads=4,
                 ff_dim=256,
                 dropout=0.1,
                 max_patches=512):
        super().__init__()

        self.patch_embed = SpecPatchEmbedding(embed_dim=embed_dim, patch_size=patch_size)

        # Learnable [CLS] token
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))

        # Positional embedding (max_patches + 1 for CLS)
        self.pos_embed = nn.Parameter(torch.zeros(1, max_patches + 1, embed_dim))

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=ff_dim,
            dropout=dropout,
            batch_first=True,
            activation="gelu"
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        self.norm = nn.LayerNorm(embed_dim)
        self.head = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(embed_dim, num_classes)
        )

        # init
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        nn.init.trunc_normal_(self.cls_token, std=0.02)

    def forward(self, x):
        """
        x: (B, n_mels, time_frames)
        """
        B = x.size(0)

        x = self.patch_embed(x)  # (B, N, D)
        N = x.size(1)

        # Add CLS token
        cls = self.cls_token.expand(B, -1, -1)  # (B, 1, D)
        x = torch.cat([cls, x], dim=1)          # (B, 1+N, D)

        # Add positional embeddings
        if (N + 1) > self.pos_embed.size(1):
            raise ValueError(f"Too many patches ({N}) for max_patches setting. "
                             f"Increase max_patches in the model.")
        x = x + self.pos_embed[:, :N+1, :]

        x = self.encoder(x)      # (B, 1+N, D)
        x = self.norm(x)

        cls_out = x[:, 0, :]     # CLS token output
        logits = self.head(cls_out)
        return logits

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Model hyperparameters (you can tune these)
embed_dim = 128
patch_size = (16, 16)
num_layers = 4
num_heads = 4
ff_dim = 256
dropout = 0.1
max_patches = 512  # should be enough for 128x~130 frames with 16x16 patches

model = SpectrogramTransformer(
    num_classes=num_classes,
    embed_dim=embed_dim,
    patch_size=patch_size,
    num_layers=num_layers,
    num_heads=num_heads,
    ff_dim=ff_dim,
    dropout=dropout,
    max_patches=max_patches
).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-2)

save_path = "/content/drive/My Drive/TUD/Projects/Music Genre Classification/best_spectrogram_transformer.pt"

patience = 7
best_val_acc = 0.0
epochs_no_improve = 0
best_state = None

def run_epoch(loader, train=True):
    model.train() if train else model.eval()

    total_loss = 0.0
    correct = 0
    total = 0

    with torch.set_grad_enabled(train):
        for xb, yb in loader:
            xb = xb.to(device, non_blocking=True)  # (B, n_mels, time)
            yb = yb.to(device, non_blocking=True)

            logits = model(xb)
            loss = criterion(logits, yb)

            if train:
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

            total_loss += loss.item() * xb.size(0)
            preds = torch.argmax(logits, dim=1)
            correct += (preds == yb).sum().item()
            total += xb.size(0)

    return total_loss / total, correct / total

num_epochs = 100

for epoch in range(1, num_epochs + 1):
    train_loss, train_acc = run_epoch(train_loader, train=True)
    val_loss, val_acc     = run_epoch(val_loader, train=False)

    print(f"Epoch {epoch:02d}/{num_epochs} "
          f"train_loss={train_loss:.4f} train_acc={train_acc:.4f} "
          f"val_loss={val_loss:.4f} val_acc={val_acc:.4f}")

    # Save best
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        best_state = copy.deepcopy(model.state_dict())
        torch.save(best_state, save_path)
        print(f"  Saved best model to: {save_path}")
        epochs_no_improve = 0
    else:
        epochs_no_improve += 1

    # Early stopping
    if epochs_no_improve >= patience:
        print(f"Early stopping triggered. Best val_acc={best_val_acc:.4f}")
        break

# Load best weights
model.load_state_dict(torch.load(save_path, map_location=device))
print("Loaded best model from:", save_path)

model.eval()
all_preds = []
all_true = []

with torch.no_grad():
    for xb, yb in test_loader:
        xb = xb.to(device, non_blocking=True)
        logits = model(xb)
        preds = torch.argmax(logits, dim=1).cpu().numpy()

        all_preds.append(preds)
        all_true.append(yb.numpy())

y_pred = np.concatenate(all_preds)
y_true = np.concatenate(all_true)

test_acc = (y_pred == y_true).mean()
print(f"Test Accuracy (Spectrogram Transformer): {test_acc:.4f}")

target_names = [idx_to_genre[i] for i in sorted(idx_to_genre)]
print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=target_names))

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(10, 8))
plt.imshow(cm, interpolation="nearest")
plt.title("Confusion Matrix (Spectrogram Transformer)")
plt.colorbar()
plt.xlabel("Predicted")
plt.ylabel("True")
plt.tight_layout()
plt.show()

!apt-get -qq update
!apt-get -qq install -y ffmpeg
!pip -q install soundfile transformers torchaudio torchcodec

import os, copy, numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

import torchaudio
from transformers import ASTFeatureExtractor, ASTForAudioClassification

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Encode labels
le = LabelEncoder()
le.fit(np.concatenate([train_labels, val_labels, test_labels]))

y_train_enc = le.transform(train_labels)
y_val_enc   = le.transform(val_labels)
y_test_enc  = le.transform(test_labels)

idx_to_genre = {i: g for i, g in enumerate(le.classes_)}
num_classes = len(le.classes_)
print("Classes:", idx_to_genre)

TARGET_SR = 16000
MAX_SECONDS = 10
MAX_SAMPLES = TARGET_SR * MAX_SECONDS

# Use torchaudio backend (ffmpeg is now installed)
# torchaudio.set_audio_backend("soundfile")  # This line is removed as set_audio_backend is deprecated

def load_audio_torchaudio(path, target_sr=TARGET_SR):
    """
    Returns 1D float32 numpy array at target_sr, or None if loading fails.
    """
    try:
        wav, sr = torchaudio.load(path)  # wav: (channels, n)
        wav = wav.mean(dim=0)            # convert to mono
        if sr != target_sr:
            wav = torchaudio.functional.resample(wav, sr, target_sr)
        return wav.numpy().astype(np.float32)
    except Exception as e:
        print(f"Error loading audio {path}: {e}") # Added for debugging
        return None


class AudioPathDataset(Dataset):
    def __init__(self, paths, labels_enc):
        self.paths = np.array(paths)
        self.labels = np.array(labels_enc)

    def __len__(self):
        return len(self.paths)

    def __getitem__(self, idx):
        path = str(self.paths[idx])
        y = int(self.labels[idx])

        audio = load_audio_torchaudio(path, TARGET_SR)

        # If load fails, return a special marker
        if audio is None or audio.size == 0:
            return None

        # Pad/trim to fixed length (10s)
        if len(audio) < MAX_SAMPLES:
            audio = np.pad(audio, (0, MAX_SAMPLES - len(audio)), mode="constant")
        else:
            audio = audio[:MAX_SAMPLES]

        return audio, y

feature_extractor = ASTFeatureExtractor.from_pretrained("MIT/ast-finetuned-audioset-10-10-0.4593")

def collate_fn(batch):
    # batch may contain None
    batch = [b for b in batch if b is not None]
    if len(batch) == 0:
        return None  # caller must handle

    audios, labels = zip(*batch)
    inputs = feature_extractor(list(audios), sampling_rate=TARGET_SR, return_tensors="pt")

    # Depending on transformers version: input_values or input_features
    if "input_values" in inputs:
        x = inputs["input_values"]
    else:
        x = inputs["input_features"]

    y = torch.tensor(labels, dtype=torch.long)
    return x, y

batch_size = 8  # AST is heavy

train_ds = AudioPathDataset(train_paths, y_train_enc)
val_ds   = AudioPathDataset(val_paths,   y_val_enc)
test_ds  = AudioPathDataset(test_paths,  y_test_enc)

train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=0, collate_fn=collate_fn)
val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)
test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)

model = ASTForAudioClassification.from_pretrained(
    "MIT/ast-finetuned-audioset-10-10-0.4593",
    num_labels=num_classes,
    ignore_mismatched_sizes=True
).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)

save_path = "/content/drive/My Drive/TUD/Projects/Music Genre Classification/best_ast_transformer.pt"

patience = 5
best_val_acc = 0.0
epochs_no_improve = 0

def run_epoch(loader, train=True):
    model.train() if train else model.eval()
    total_loss, correct, total = 0.0, 0, 0

    with torch.set_grad_enabled(train):
        for batch in loader:
            if batch is None:
                continue  # all items in this batch failed to load

            xb, yb = batch
            xb = xb.to(device, non_blocking=True)
            yb = yb.to(device, non_blocking=True)

            # Corrected: Always pass as input_values, as AST model expects spectrograms here.
            out = model(input_values=xb)
            logits = out.logits
            loss = criterion(logits, yb)

            if train:
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()

            total_loss += loss.item() * xb.size(0)
            preds = torch.argmax(logits, dim=1)
            correct += (preds == yb).sum().item()
            total += xb.size(0)

    if total == 0:
        return float("inf"), 0.0

    return total_loss / total, correct / total


num_epochs = 20

for epoch in range(1, num_epochs + 1):
    train_loss, train_acc = run_epoch(train_loader, train=True)
    val_loss, val_acc     = run_epoch(val_loader, train=False)

    print(f"Epoch {epoch:02d}/{num_epochs} "
          f"train_loss={train_loss:.4f} train_acc={train_acc:.4f} "
          f"val_loss={val_loss:.4f} val_acc={val_acc:.4f}")

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), save_path)
        print(f"  Saved best model to: {save_path}")
        epochs_no_improve = 0
    else:
        epochs_no_improve += 1

    if epochs_no_improve >= patience:
        print(f"Early stopping. Best val_acc={best_val_acc:.4f}")
        break

# Load best weights
model.load_state_dict(torch.load(save_path, map_location=device))
print("Loaded best model from:", save_path)

model.eval()
all_preds = []
all_true = []

with torch.no_grad():
    for xb, yb in test_loader:
        xb = xb.to(device, non_blocking=True)
        # Correctly access the logits tensor from the model's output
        logits = model(xb).logits
        preds = torch.argmax(logits, dim=1).cpu().numpy()

        all_preds.append(preds)
        all_true.append(yb.numpy())

y_pred = np.concatenate(all_preds)
y_true = np.concatenate(all_true)

test_acc = (y_pred == y_true).mean()
print(f"Test Accuracy (Spectrogram Transformer): {test_acc:.4f}")

target_names = [idx_to_genre[i] for i in sorted(idx_to_genre)]
print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=target_names))

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(10, 8))
plt.imshow(cm, interpolation="nearest")
plt.title("Confusion Matrix (Spectrogram Transformer)")
plt.colorbar()
plt.xlabel("Predicted")
plt.ylabel("True")
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np

# Create train.csv
train_df = pd.DataFrame({
    "filepath": train_paths,
    "label": y_train_enc
})
train_df.to_csv("train.csv", index=False)

# Create val.csv
val_df = pd.DataFrame({
    "filepath": val_paths,
    "label": y_val_enc
})
val_df.to_csv("val.csv", index=False)

# Create test.csv
test_df = pd.DataFrame({
    "filepath": test_paths,
    "label": y_test_enc
})
test_df.to_csv("test.csv", index=False)

print("Created train.csv, val.csv, and test.csv")
print(f"train.csv head:\n{train_df.head()}")
print(f"val.csv head:\n{val_df.head()}")
print(f"test.csv head:\n{test_df.head()}")

# AST (Audio Spectrogram Transformer) end-to-end example
# - Loads pretrained: MIT/ast-finetuned-audioset-10-10-0.4593
# - Builds a simple dataset from a CSV: filepath,label
# - Trains (optional) + Evaluates + Runs inference
#
# Install (if needed):
#   pip install torch torchaudio transformers accelerate scikit-learn pandas tqdm

import os
import math
import random
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

import torchaudio
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import accuracy_score, classification_report

from transformers import (
    ASTFeatureExtractor,
    ASTForAudioClassification,
)
from transformers.optimization import get_linear_schedule_with_warmup
from torch.optim import AdamW # Corrected import to use PyTorch's AdamW

# -----------------------------
# Config
# -----------------------------
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
PRETRAINED_CKPT = "MIT/ast-finetuned-audioset-10-10-0.4593"

SEED = 42
BATCH_SIZE = 8
EPOCHS = 3
LR = 2e-5
WEIGHT_DECAY = 0.01
MAX_AUDIO_SECONDS = 10.0  # AST checkpoint expects ~10 sec clips (AudioSet fine-tuning)
TARGET_SAMPLE_RATE = 16000

# CSV format: filepath,label (label is integer class id)
TRAIN_CSV = "train.csv"
VAL_CSV = "val.csv"
TEST_CSV = "test.csv"

# -----------------------------
# Reproducibility
# -----------------------------
def seed_everything(seed: int = 42) -> None:
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = False
    torch.backends.cudnn.benchmark = True

seed_everything(SEED)

# -----------------------------
# Audio helpers
# -----------------------------
def load_audio_mono_resample(path: str, target_sr: int = 16000) -> Optional[Tuple[torch.Tensor, int]]:
    """
    Returns:
      waveform: Tensor shape [num_samples] (mono)
      sr: sample_rate
      or None if loading fails
    """
    try:
        waveform, sr = torchaudio.load(path)  # [channels, samples]
        if waveform.ndim != 2:
            # Handle cases where waveform might be 1D or have unexpected shape
            waveform = waveform.unsqueeze(0) # Make it [1, samples] if it's 1D
            # raise ValueError(f"Unexpected waveform shape: {waveform.shape}")

        # Convert to mono
        if waveform.size(0) > 1:
            waveform = waveform.mean(dim=0, keepdim=True)
        waveform = waveform.squeeze(0)  # [samples]

        # Resample
        if sr != target_sr:
            waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)
            sr = target_sr

        return waveform, sr
    except Exception as e:
        # print(f"Error loading audio {path}: {e}") # For debugging
        return None


def pad_or_trim(waveform: torch.Tensor, sr: int, max_seconds: float) -> torch.Tensor:
    """
    Ensures a fixed length waveform (AST often trained on ~10 sec).
    """
    max_len = int(sr * max_seconds)
    if waveform.numel() == 0:
        return torch.zeros(max_len, dtype=torch.float32)

    if waveform.numel() > max_len:
        return waveform[:max_len]
    if waveform.numel() < max_len:
        pad_len = max_len - waveform.numel()
        return torch.nn.functional.pad(waveform, (0, pad_len))
    return waveform


# -----------------------------
# Dataset
# -----------------------------
class CSVAudioDataset(Dataset):
    def __init__(
        self,
        csv_path: str,
        max_seconds: float = 10.0,
        target_sr: int = 16000,
    ):
        self.df = pd.read_csv(csv_path)
        if "filepath" not in self.df.columns or "label" not in self.df.columns:
            raise ValueError("CSV must contain columns: filepath,label")
        self.max_seconds = max_seconds
        self.target_sr = target_sr

    def __len__(self) -> int:
        return len(self.df)

    def __getitem__(self, idx: int) -> Optional[Dict[str, Any]]:
        row = self.df.iloc[idx]
        path = str(row["filepath"])
        label = int(row["label"])

        load_result = load_audio_mono_resample(path, self.target_sr)
        if load_result is None:
            return None # Skip problematic files
        waveform, sr = load_result

        waveform = pad_or_trim(waveform, sr, self.max_seconds)

        return {"waveform": waveform, "label": label, "path": path}


# -----------------------------
# Collator for AST
# -----------------------------
@dataclass
class ASTCollator:
    feature_extractor: ASTFeatureExtractor
    sampling_rate: int = 16000

    def __call__(self, batch: List[Optional[Dict[str, Any]]]) -> Dict[str, torch.Tensor]:
        # Filter out None entries (from problematic audio files)
        batch = [b for b in batch if b is not None]
        if not batch: # If all items in batch were None
            return {} # Return an empty dictionary, DataLoader will handle it

        waveforms = [b["waveform"].numpy() for b in batch]  # feature_extractor expects numpy
        labels = torch.tensor([b["label"] for b in batch], dtype=torch.long)

        feats = self.feature_extractor(
            waveforms,
            sampling_rate=self.sampling_rate,
            return_tensors="pt",
        )
        # feats contains: input_values (log-mel features prepared for AST)
        feats["labels"] = labels
        return feats


# -----------------------------
# Model load
# -----------------------------
def load_ast_model(num_classes: int) -> Tuple[ASTForAudioClassification, ASTFeatureExtractor]:
    feature_extractor = ASTFeatureExtractor.from_pretrained(PRETRAINED_CKPT)
    model = ASTForAudioClassification.from_pretrained(
        PRETRAINED_CKPT,
        num_labels=num_classes,
        ignore_mismatched_sizes=True,  # important if your num_classes != pretrained head
    )
    return model.to(DEVICE), feature_extractor


# -----------------------------
# Train / Eval
# -----------------------------
@torch.no_grad()
def evaluate(model: nn.Module, loader: DataLoader) -> Tuple[float, List[int], List[int]]:
    model.eval()
    y_true, y_pred = [], []

    for batch in tqdm(loader, desc="Eval", leave=False):
        if not batch: continue # Skip empty batches from collator
        batch = {k: v.to(DEVICE) for k, v in batch.items()}
        labels = batch.pop("labels")
        outputs = model(**batch)
        logits = outputs.logits
        preds = torch.argmax(logits, dim=-1)

        y_true.extend(labels.cpu().tolist())
        y_pred.extend(preds.cpu().tolist())

    acc = accuracy_score(y_true, y_pred) if len(y_true) else 0.0
    return acc, y_true, y_pred


def train_one_epoch(
    model: nn.Module,
    loader: DataLoader,
    optimizer: torch.optim.Optimizer,
    scheduler,
    grad_clip: Optional[float] = 1.0,
) -> float:
    model.train()
    losses = []

    for batch in tqdm(loader, desc="Train", leave=False):
        if not batch: continue # Skip empty batches from collator
        batch = {k: v.to(DEVICE) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss

        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        if grad_clip is not None:
            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
        optimizer.step()
        if scheduler is not None:
            scheduler.step()

        losses.append(loss.item())

    return float(sum(losses) / max(1, len(losses)))


# -----------------------------
# Main
# -----------------------------
def main():
    # Infer number of classes from train.csv
    train_df = pd.read_csv(TRAIN_CSV)
    num_classes = int(train_df["label"].nunique())

    model, feature_extractor = load_ast_model(num_classes=num_classes)
    collator = ASTCollator(feature_extractor=feature_extractor, sampling_rate=TARGET_SAMPLE_RATE)

    train_ds = CSVAudioDataset(TRAIN_CSV, max_seconds=MAX_AUDIO_SECONDS, target_sr=TARGET_SAMPLE_RATE)
    val_ds = CSVAudioDataset(VAL_CSV, max_seconds=MAX_AUDIO_SECONDS, target_sr=TARGET_SAMPLE_RATE)
    test_ds = CSVAudioDataset(TEST_CSV, max_seconds=MAX_AUDIO_SECONDS, target_sr=TARGET_SAMPLE_RATE)

    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=collator)
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, collate_fn=collator)
    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, collate_fn=collator)

    # Optimizer + schedule
    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
    total_steps = EPOCHS * max(1, len(train_loader))
    warmup_steps = int(0.1 * total_steps)
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps,
    )

    best_val_acc = -1.0
    best_path = "best_ast.pt"

    for epoch in range(1, EPOCHS + 1):
        train_loss = train_one_epoch(model, train_loader, optimizer, scheduler)
        val_acc, _, _ = evaluate(model, val_loader)

        print(f"Epoch {epoch}/{EPOCHS} | train_loss={train_loss:.4f} | val_acc={val_acc:.4f}")

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), best_path)

    # Load best + test
    model.load_state_dict(torch.load(best_path, map_location=DEVICE))
    test_acc, y_true, y_pred = evaluate(model, test_loader)

    print(f"\nBest val_acc={best_val_acc:.4f}")
    print(f"Test Accuracy={test_acc:.4f}")
    print("\nClassification report:")
    print(classification_report(y_true, y_pred, digits=4))

    # -------------------------
    # Single-file inference demo
    # -------------------------
    example_path = test_df_first_path(TEST_CSV)
    pred = predict_one_file(model, feature_extractor, example_path)
    print(f"\nExample inference:\n  file={example_path}\n  predicted_class={pred}")


def test_df_first_path(csv_path: str) -> str:
    df = pd.read_csv(csv_path)
    return str(df.iloc[0]["filepath"])


@torch.no_grad()
def predict_one_file(
    model: ASTForAudioClassification,
    feature_extractor: ASTFeatureExtractor,
    audio_path: str,
) -> int:
    model.eval()
    waveform, sr = load_audio_mono_resample(audio_path, TARGET_SAMPLE_RATE)
    if waveform is None: # Handle case where single file inference fails to load audio
        print(f"Warning: Could not load audio for inference: {audio_path}")
        return -1 # Or raise an error, or return a special value

    waveform = pad_or_trim(waveform, sr, MAX_AUDIO_SECONDS)

    feats = feature_extractor(
        [waveform.numpy()],
        sampling_rate=TARGET_SAMPLE_RATE,
        return_tensors="pt",
    )
    feats = {k: v.to(DEVICE) for k, v in feats.items()}

    logits = model(**feats).logits
    pred = int(torch.argmax(logits, dim=-1).item())
    return pred


if __name__ == "__main__":
    main()