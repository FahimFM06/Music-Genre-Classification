# ğŸµ Music Genre Classification using Deep Learning

This project implements a **complete end-to-end Music Genre Classification system** using audio signal processing and deep learning.  
Multiple architectures are explored and compared, including:

- **CRNN (CNN + Bi-LSTM) â€“ TensorFlow/Keras**
- **CRNN + BiGRU â€“ PyTorch**
- **Spectrogram Transformer (ViT-style) â€“ PyTorch**
- **Explainable AI (XAI)** using Integrated Gradients, Grad-CAM, and Occlusion

The project is built on the **GTZAN music genre dataset** and follows a clean, research-oriented pipeline.

---

## ğŸ“Œ Project Highlights

- ğŸ§ Raw audio processing with **Librosa**
- ğŸ“Š Extensive **Exploratory Data Analysis (EDA)**
- ğŸ¼ Log-Mel Spectrogram feature extraction
- ğŸ§  Multiple deep learning architectures (CNN, RNN, Transformer)
- ğŸ” Explainable AI (XAI) for model interpretability
- âš¡ GPU-accelerated training (PyTorch & TensorFlow)
- ğŸŒ Streamlit-ready inference pipeline

---

## ğŸ—‚ï¸ Dataset

- **Dataset:** GTZAN Music Genre Dataset  
- **Source:** https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification  
- **Genres (10):**  
- **blues, classical, country, disco, hiphop,jazz, metal, pop, reggae, rock.**
- - **Audio format:** WAV  
- **Duration:** ~30 seconds per track  
- **Sample rate:** 22,050 Hz  



---

## ğŸ—ï¸ Project Pipeline

### 1ï¸âƒ£ Audio Loading
- Loads WAV files genre-wise
- Preserves original sample rate
- Handles corrupted audio safely

### 2ï¸âƒ£ Exploratory Data Analysis (EDA)
- Waveform visualization
- Genre distribution analysis
- Sample rate consistency check
- Track duration statistics
- Boxplots by genre
- Mel-spectrogram visualization

### 3ï¸âƒ£ Audio Preprocessing
- Resample to **22,050 Hz**
- Convert to mono
- Fix length to **30 seconds**
- Split into **10 segments (3s each)**

### 4ï¸âƒ£ Feature Extraction
- **Log-Mel Spectrogram**
- `n_mels = 128`
- `n_fft = 2048`
- `hop_length = 512`
- Per-segment normalization (zero mean, unit variance)
- Final input shape: **(128 Ã— 130)**

---

## ğŸ§  Models Implemented

### ğŸ”¹ 1. CRNN (TensorFlow / Keras)

**Architecture:**
- 3 Ã— Conv2D + BatchNorm + MaxPooling
- Reshape CNN output â†’ sequence
- **Bidirectional LSTM**
- Dense classification head

**Performance:**
- âœ… Test Accuracy: **~72%**

---

### ğŸ”¹ 2. CRNN + BiGRU (PyTorch)

**Architecture:**
- CNN feature extractor
- Dynamic CNN output inference
- **Bidirectional GRU**
- Fully-connected classifier

**Performance:**
- âœ… Test Accuracy: **~79%**

This is the **best-performing model** in the project.

---

### ğŸ”¹ 3. Spectrogram Transformer (PyTorch)

**Architecture:**
- Patch embedding of spectrograms
- Learnable `[CLS]` token
- Positional embeddings
- Transformer Encoder (Multi-Head Attention)
- Classification head

**Performance:**
- âœ… Test Accuracy: **~64%**

---

## ğŸ“Š Model Comparison

| Model | Framework | Test Accuracy |
|------|----------|---------------|
| CRNN (CNN + Bi-LSTM) | TensorFlow | ~72% |
| **CRNN + BiGRU** | **PyTorch** | **~79%** |
| Spectrogram Transformer | PyTorch | ~64% |

---

## ğŸ” Explainable AI (XAI)

To understand **why** the model makes predictions, multiple XAI techniques are used:

### âœ… Integrated Gradients
- Highlights important time-frequency regions
- Shows contribution of spectrogram bins

### âœ… Grad-CAM
- Visualizes CNN attention regions
- Heatmap over spectrogram

### âœ… Occlusion Sensitivity
- Measures prediction sensitivity to masked regions

These methods improve **model transparency and trustworthiness**.

---

## ğŸ–¥ï¸ Technologies Used

- **Python**
- **Librosa**
- **NumPy / Pandas**
- **Matplotlib / Seaborn**
- **TensorFlow / Keras**
- **PyTorch**
- **Scikit-learn**
- **Captum (XAI)**
---

## ğŸš€ Streamlit Web Application (GUI)

This project includes an interactive **Streamlit-based web application** that allows users to upload audio files and receive real-time music genre predictions using the trained **CRNN + BiGRU (PyTorch)** model.

ğŸ”— **Live App Link:**  
https://music-genre-classification-crnn-bigru.streamlit.app/

### ğŸ›ï¸ GUI Features
- Audio upload with playback
- Device information (CPU / GPU)
- Input spectrogram size display
- Adjustable inference settings
- Top-K genre probability visualization
- Segment-wise prediction analysis
- Waveform and log-mel spectrogram visualization

---

## ğŸ–¥ï¸ Streamlit App Layout

### ğŸ”¹ Sidebar Controls
- **Navigation**
  - Dashboard
  - How it works
  - About
- **Inference Settings**
  - Aggregation method (Mean / Median / Max)
  - Top-K predictions slider
  - Toggle:
    - Per-segment prediction table
    - Waveform visualization
    - Mel-spectrogram preview

### ğŸ”¹ Main Dashboard
- Displays:
  - Running device (CPU/GPU)
  - Input feature size (`128 Ã— 130`)
  - Number of segments per track (`10`)
- Audio file uploader with supported formats:



---

## ğŸ›ï¸ Audio Preprocessing Pipeline

The following preprocessing steps are applied before inference:

1. Resample audio to **22,050 Hz**
2. Convert to **mono**
3. Standardize audio length to **30 seconds**
4. Split audio into **10 segments**, each **3 seconds**
5. For each segment:
 - Compute **Log-Mel Spectrogram**
   - `n_mels = 128`
   - `n_fft = 2048`
   - `hop_length = 512`
 - Normalize features (zero mean, unit variance)
6. Run inference on each segment
7. Aggregate predictions to produce the final genre

**Final input shape per segment:**  

