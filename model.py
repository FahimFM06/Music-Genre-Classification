# -*- coding: utf-8 -*-
"""model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-CWPtFn-Epyu6AyqoLpwJf2RWJ_50GgK
"""

import torch
import torch.nn as nn


class CRNN_BiGRU(nn.Module):
    """
    Must match the training architecture used to create best_crnn_bigru_pytorch.pt
    Input: (B, n_mels, time_frames)
    """

    def __init__(
        self,
        num_classes: int,
        n_mels: int = 128,
        time_frames: int = 130,
        rnn_hidden: int = 128,
        dropout_cnn: float = 0.35,
    ):
        super().__init__()

        # EXACT layer order from your notebook (important for state_dict keys)
        self.cnn = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, padding=1),   # 0
            nn.BatchNorm2d(32),                            # 1
            nn.ReLU(),                                     # 2
            nn.MaxPool2d((2, 2)),                          # 3
            nn.Dropout(0.25),                              # 4

            nn.Conv2d(32, 64, kernel_size=3, padding=1),   # 5
            nn.BatchNorm2d(64),                            # 6
            nn.ReLU(),                                     # 7
            nn.MaxPool2d((2, 2)),                          # 8
            nn.Dropout(0.30),                              # 9

            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # 10
            nn.BatchNorm2d(128),                           # 11
            nn.ReLU(),                                     # 12
            nn.MaxPool2d((2, 2)),                          # 13
            nn.Dropout(dropout_cnn),                       # 14
        )

        # Infer GRU input size exactly like training
        with torch.no_grad():
            dummy = torch.zeros(1, 1, n_mels, time_frames)
            out = self.cnn(dummy)
            _, c, h, w = out.shape
            gru_input_size = c * h

        self.gru = nn.GRU(
            input_size=gru_input_size,
            hidden_size=rnn_hidden,
            num_layers=1,
            batch_first=True,
            bidirectional=True,
        )

        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(2 * rnn_hidden, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, num_classes),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B, n_mels, time_frames)
        x = x.unsqueeze(1)          # (B, 1, n_mels, time)
        x = self.cnn(x)             # (B, C, H, W)

        b, c, h, w = x.shape
        x = x.permute(0, 3, 1, 2).contiguous()  # (B, W, C, H)
        x = x.view(b, w, c * h)                  # (B, W, C*H)

        out, _ = self.gru(x)          # (B, W, 2*rnn_hidden)
        out_last = out[:, -1, :]      # (B, 2*rnn_hidden)
        logits = self.classifier(out_last)
        return logits